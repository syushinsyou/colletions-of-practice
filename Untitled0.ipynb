{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HLOWUJNpRcFN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "N_CLASSES = 10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxIX2Y-MT06S",
        "outputId": "0c6405ae-0e3c-4d37-9337-a0e7c378fc1b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DenseNet121(nn.Module):\n",
        "  def __init__(self, out_size):\n",
        "      super(DenseNet121, self).__init__()\n",
        "      self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
        "      num_ftrs = self.densenet121.classifier.in_features\n",
        "      self.densenet121.classifier = nn.Sequential(\n",
        "          nn.Linear(num_ftrs, out_size),\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.densenet121(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "aGgP2V_XN--g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "densenet = DenseNet121(N_CLASSES).cuda()\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "optimizer = torch.optim.Adam(densenet.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14jiI_mvT01X",
        "outputId": "50f30e1d-f9fc-4ee7-c861-32b4301a7804"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 167MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(batch_size):  # loop over the dataset multiple times\n",
        "    densenet.train()\n",
        "    running_loss = 0.0\n",
        "    #for i, (inputs, target) in enumerate(trainloader, 0):\n",
        "    for i, (inputs, target) in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch+1}\")):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, target = inputs.cuda(), target.cuda()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = densenet(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    validate(testloader, densenet, criterion)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "def validate(testloader, densenet, criterion):\n",
        "\n",
        "  densenet.eval()\n",
        "  val_loss = 0.0\n",
        "  gt = []\n",
        "  pred = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (inputs, target) in enumerate(tqdm(testloader, desc=\"Validation\")):\n",
        "        inputs, target = inputs.cuda(), target.cuda()\n",
        "        outputs = densenet(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "                # 将目标和预测值添加到列表中\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        gt.append(target.cpu())\n",
        "        pred.append(outputs.cpu())\n",
        "\n",
        "        # gt = torch.cat((gt, target), 0)\n",
        "        # pred = torch.cat((pred, outputs), 0)\n",
        "            # 转换为 NumPy 数组\n",
        "    gt = torch.cat(gt, dim=0)\n",
        "    pred = torch.cat(pred, dim=0)\n",
        "\n",
        "    gt_np = gt.numpy()\n",
        "    pred_np = pred.numpy()\n",
        "    if len(gt_np.shape) == 1:\n",
        "        gt_np = np.eye(N_CLASSES)[gt_np]\n",
        "\n",
        "  # AUROCs = compute_AUCs(gt, pred)\n",
        "  # AUROC_avg = np.array(AUROCs).mean()\n",
        "    AUROCs = []\n",
        "    for i in range(N_CLASSES):\n",
        "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
        "    AUROC_avg = np.array(AUROCs).mean()\n",
        "\n",
        "  print(f'Validation Loss: {val_loss / len(testloader):.4f}, Average AUROC: {AUROC_avg:.3f}')\n",
        "  for i in range(N_CLASSES):\n",
        "      print(f'The AUROC of {classes[i]} is {AUROCs[i]:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "4D22hCS4T0vI",
        "outputId": "129ff6b6-736b-4c01-d599-14e0a2a86726"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  97%|█████████▋| 95/98 [00:20<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-24d095d9fb6c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1999\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# print every 2000 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation step after each epoch\n",
        "# validate(testloader, densenet, criterion)\n",
        "\n",
        "# Save the model checkpoint\n",
        "PATH = './cifar_densenet.path'\n",
        "torch.save({'state_dict': densenet.state_dict()}, PATH)"
      ],
      "metadata": {
        "id": "u705l-gZo8zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "def compute_AUCs(gt, pred):\n",
        "    \"\"\"Computes Area Under the Curve (AUC) from prediction scores.\n",
        "\n",
        "    Args:\n",
        "        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
        "          true binary labels.\n",
        "        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
        "          can either be probability estimates of the positive class,\n",
        "          confidence values, or binary decisions.\n",
        "\n",
        "    Returns:\n",
        "        List of AUROCs of all classes.\n",
        "    \"\"\"\n",
        "    AUROCs = []\n",
        "    gt_np = gt.cpu().numpy()\n",
        "    pred_np = pred.cpu().numpy()\n",
        "    # gt = gt.cpu().numpy()\n",
        "    # pred = pred.cpu().numpy()\n",
        "    multi_class='ovr'\n",
        "\n",
        "    for i in range(N_CLASSES):\n",
        "        # print(gt_np[:, i], pred_np[:, i])\n",
        "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
        "        #AUROCs.append(roc_auc_score(gt_np[0], pred_np[0]))\n",
        "\n",
        "    return AUROCs"
      ],
      "metadata": {
        "id": "OLv_0lXSp-Jn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # initialize the ground truth and output tensor\n",
        "# gt = torch.FloatTensor()\n",
        "# gt = gt.cuda()\n",
        "# pred = torch.FloatTensor()\n",
        "# pred = pred.cuda()\n",
        "\n",
        "# densenet.eval()\n",
        "# for i, (inp, target) in enumerate(testloader):\n",
        "#     target = target.cuda()\n",
        "#     gt = torch.cat((gt, target), 0)\n",
        "#     bs, n_crops, c, h, w = inp.size()\n",
        "#     input_var = torch.autograd.Variable(inp.view(-1, c, h, w).cuda(), volatile=True)\n",
        "#     output = densenet(input_var)\n",
        "#     output_mean = output.view(bs, n_crops, -1).mean(1)\n",
        "#     pred = torch.cat((pred, output_mean.data), 0)\n",
        "\n",
        "# AUROCs = compute_AUCs(gt, pred)\n",
        "# AUROC_avg = np.array(AUROCs).mean()\n",
        "# print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
        "# for i in range(N_CLASSES):\n",
        "#     print('The AUROC of {} is {}'.format(classes[i], AUROCs[i]))\n"
      ],
      "metadata": {
        "id": "x1kcRfLS0cCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(testloader, densenet, criterion):\n",
        "\n",
        "  densenet.eval()\n",
        "  val_loss = 0.0\n",
        "  gt = []\n",
        "  pred = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (inputs, target) in enumerate(tqdm(testloader, desc=\"Validation\")):\n",
        "        inputs, target = inputs.cuda(), target.cuda()\n",
        "        outputs = densenet(inputs)\n",
        "        loss = criterion(outputs, target)\n",
        "                # 将目标和预测值添加到列表中\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        gt.append(target.cpu())\n",
        "        pred.append(outputs.cpu())\n",
        "\n",
        "        # gt = torch.cat((gt, target), 0)\n",
        "        # pred = torch.cat((pred, outputs), 0)\n",
        "            # 转换为 NumPy 数组\n",
        "    gt = torch.cat(gt, dim=0)\n",
        "    pred = torch.cat(pred, dim=0)\n",
        "\n",
        "    gt_np = gt.numpy()\n",
        "    pred_np = pred.numpy()\n",
        "    if len(gt_np.shape) == 1:\n",
        "        gt_np = np.eye(N_CLASSES)[gt_np]\n",
        "\n",
        "  # AUROCs = compute_AUCs(gt, pred)\n",
        "  # AUROC_avg = np.array(AUROCs).mean()\n",
        "    AUROCs = []\n",
        "    for i in range(N_CLASSES):\n",
        "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
        "    AUROC_avg = np.array(AUROCs).mean()\n",
        "\n",
        "  print(f'Validation Loss: {val_loss / len(testloader):.4f}, Average AUROC: {AUROC_avg:.3f}')\n",
        "  for i in range(N_CLASSES):\n",
        "      print(f'The AUROC of {classes[i]} is {AUROCs[i]:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "sN7RGtXXT_e7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate(testloader, densenet, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2tKeZxWkx5R",
        "outputId": "03d3db1b-d46e-4bec-c258-abff0621f3cd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 20/20 [00:02<00:00,  7.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.9944, Average AUROC: 0.781\n",
            "The AUROC of plane is 0.815\n",
            "The AUROC of car is 0.873\n",
            "The AUROC of bird is 0.676\n",
            "The AUROC of cat is 0.705\n",
            "The AUROC of deer is 0.712\n",
            "The AUROC of dog is 0.747\n",
            "The AUROC of frog is 0.739\n",
            "The AUROC of horse is 0.805\n",
            "The AUROC of ship is 0.877\n",
            "The AUROC of truck is 0.857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def validate(testloader, densenet, criterion):\n",
        "    densenet.eval()\n",
        "    val_loss = 0.0\n",
        "    gt = []  # 使用列表来存储每个 batch 的目标\n",
        "    pred = []  # 使用列表来存储每个 batch 的预测\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(tqdm(testloader, desc=\"Validation\")):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            outputs = densenet(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # 将目标和预测值添加到列表中\n",
        "            gt.append(targets.cpu())\n",
        "            pred.append(outputs.cpu())\n",
        "\n",
        "    # 将列表中的张量拼接成一个整体张量\n",
        "    gt = torch.cat(gt, dim=0)\n",
        "    pred = torch.cat(pred, dim=0)\n",
        "\n",
        "    # 转换为 NumPy 数组\n",
        "    gt_np = gt.numpy()\n",
        "    pred_np = pred.numpy()\n",
        "\n",
        "    # 如果是多类分类任务，将标签转换为 one-hot 编码\n",
        "    if len(gt_np.shape) == 1:\n",
        "        gt_np = np.eye(N_CLASSES)[gt_np]\n",
        "\n",
        "    # 计算 AUROC 分数\n",
        "    AUROCs = []\n",
        "    for i in range(N_CLASSES):\n",
        "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
        "    AUROC_avg = np.array(AUROCs).mean()\n",
        "\n",
        "    # 打印验证损失和 AUROC\n",
        "    print(f'Validation Loss: {val_loss / len(testloader):.4f}, Average AUROC: {AUROC_avg:.3f}')\n",
        "    for i in range(N_CLASSES):\n",
        "        print(f'The AUROC of {classes[i]} is {AUROCs[i]:.3f}')\n"
      ],
      "metadata": {
        "id": "js79G7JJbsvv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate(testloader, densenet, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARx1FP-yxov3",
        "outputId": "23d42357-9b2a-4d97-9d2a-24fa180ca4f7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 2500/2500 [00:50<00:00, 49.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.3133, Average AUROC: 0.510\n",
            "The AUROC of plane is 0.462\n",
            "The AUROC of car is 0.420\n",
            "The AUROC of bird is 0.549\n",
            "The AUROC of cat is 0.586\n",
            "The AUROC of deer is 0.578\n",
            "The AUROC of dog is 0.523\n",
            "The AUROC of frog is 0.507\n",
            "The AUROC of horse is 0.417\n",
            "The AUROC of ship is 0.690\n",
            "The AUROC of truck is 0.372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGnHqpqVNmlb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision import models\n",
        "from dataloader import ChestXrayDataSet\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
        "#                 'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "\n",
        "# DATA_DIR = '/home/jzhu/v2-sdlora/generativeprocessing/?generated_images'\n",
        "# TRAIN_IMAGE_LIST = '/content/sample_data/california_housing_train.csv'\n",
        "# VAL_IMAGE_LIST = '/content/sample_data/california_housing_test.csv'\n",
        "CKPT_PATH = 'model.pth.tar'\n",
        "\n",
        "N_CLASSES = 14\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "def main():\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    # 数据增强和预处理\n",
        "    transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])\n",
        "                                        ])\n",
        "\n",
        "    train_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n",
        "                                    image_list_file=TRAIN_IMAGE_LIST,\n",
        "                                    transform=transform)\n",
        "    val_dataset = ChestXrayDataSet(data_dir=DATA_DIR,\n",
        "                                   image_list_file=VAL_IMAGE_LIST,\n",
        "                                   transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              pin_memory=True)\n",
        "\n",
        "    val_loader = DataLoader(dataset=val_dataset,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            shuffle=False,\n",
        "                            num_workers=8,\n",
        "                            pin_memory=True)\n",
        "\n",
        "print(f\"Length of train_loader: {len(train_loader)}\")\n",
        "print(f\"Length of val_loader: {len(val_loader)}\")\n",
        "\n",
        "print(f\"Number of samples in dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of samples in dataset: {len(val_dataset)}\")\n"
      ]
    }
  ]
}